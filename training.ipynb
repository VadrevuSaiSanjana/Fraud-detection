{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ee6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# %pip install pandas numpy scikit-learn xgboost lightgbm imbalanced-learn joblib matplotlib seaborn tensorflow shap\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9679ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PotentialFraud\n",
       "0    0.90647\n",
       "1    0.09353\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load features and target\n",
    "features = pd.read_csv(r'C:\\Users\\gunar\\OneDrive\\Desktop\\fraud detection\\Features.csv')\n",
    "target = pd.read_csv(r'C:\\Users\\gunar\\OneDrive\\Desktop\\fraud detection\\Target.csv')\n",
    "\n",
    "\n",
    "# Merge for convenience\n",
    "df = pd.concat([features, target], axis=1)\n",
    "df.head()\n",
    "\n",
    "# Missing values\n",
    "df.isnull().sum()[lambda x: x>0]\n",
    "\n",
    "# Class balance\n",
    "df['PotentialFraud'].value_counts(), \n",
    "df['PotentialFraud'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417da3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['PotentialFraud'])\n",
    "y = df['PotentialFraud']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0187d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\n{name}\")\n",
    "    print(classification_report(y_test, preds, digits=4))\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, preds),\n",
    "        'Precision': precision_score(y_test, preds),\n",
    "        'Recall': recall_score(y_test, preds),\n",
    "        'F1-Score': f1_score(y_test, preds)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1108a814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9859    0.9246    0.9542       981\n",
      "           1     0.5432    0.8713    0.6692       101\n",
      "\n",
      "    accuracy                         0.9196      1082\n",
      "   macro avg     0.7645    0.8979    0.8117      1082\n",
      "weighted avg     0.9445    0.9196    0.9276      1082\n",
      "\n",
      "Confusion Matrix:\n",
      " [[907  74]\n",
      " [ 13  88]]\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9536    0.9847    0.9689       981\n",
      "           1     0.7826    0.5347    0.6353       101\n",
      "\n",
      "    accuracy                         0.9427      1082\n",
      "   macro avg     0.8681    0.7597    0.8021      1082\n",
      "weighted avg     0.9376    0.9427    0.9378      1082\n",
      "\n",
      "Confusion Matrix:\n",
      " [[966  15]\n",
      " [ 47  54]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunar\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [11:24:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9622    0.9602    0.9612       981\n",
      "           1     0.6214    0.6337    0.6275       101\n",
      "\n",
      "    accuracy                         0.9298      1082\n",
      "   macro avg     0.7918    0.7970    0.7943      1082\n",
      "weighted avg     0.9304    0.9298    0.9301      1082\n",
      "\n",
      "Confusion Matrix:\n",
      " [[942  39]\n",
      " [ 37  64]]\n",
      "[LightGBM] [Info] Number of positive: 405, number of negative: 3923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57371\n",
      "[LightGBM] [Info] Number of data points in the train set: 4328, number of used features: 300\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093577 -> initscore=-2.270725\n",
      "[LightGBM] [Info] Start training from score -2.270725\n",
      "\n",
      "LightGBM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9669    0.9531    0.9600       981\n",
      "           1     0.6000    0.6832    0.6389       101\n",
      "\n",
      "    accuracy                         0.9279      1082\n",
      "   macro avg     0.7835    0.8181    0.7994      1082\n",
      "weighted avg     0.9327    0.9279    0.9300      1082\n",
      "\n",
      "Confusion Matrix:\n",
      " [[935  46]\n",
      " [ 32  69]]\n",
      "\n",
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9565    0.9409    0.9486       981\n",
      "           1     0.5043    0.5842    0.5413       101\n",
      "\n",
      "    accuracy                         0.9076      1082\n",
      "   macro avg     0.7304    0.7625    0.7449      1082\n",
      "weighted avg     0.9143    0.9076    0.9106      1082\n",
      "\n",
      "Confusion Matrix:\n",
      " [[923  58]\n",
      " [ 42  59]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunar\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [11:24:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voting Ensemble\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9583    0.9837    0.9708       981\n",
      "           1     0.7867    0.5842    0.6705       101\n",
      "\n",
      "    accuracy                         0.9464      1082\n",
      "   macro avg     0.8725    0.7839    0.8206      1082\n",
      "weighted avg     0.9423    0.9464    0.9428      1082\n",
      "\n",
      "Confusion Matrix:\n",
      " [[965  16]\n",
      " [ 42  59]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunar\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [11:24:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\gunar\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [11:25:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\gunar\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [11:25:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\gunar\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [11:25:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\gunar\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [11:25:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\gunar\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [11:25:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacking Ensemble\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9564    0.9837    0.9698       981\n",
      "           1     0.7808    0.5644    0.6552       101\n",
      "\n",
      "    accuracy                         0.9445      1082\n",
      "   macro avg     0.8686    0.7740    0.8125      1082\n",
      "weighted avg     0.9400    0.9445    0.9405      1082\n",
      "\n",
      "Confusion Matrix:\n",
      " [[965  16]\n",
      " [ 44  57]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Voting Ensemble</td>\n",
       "      <td>0.946396</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.584158</td>\n",
       "      <td>0.670455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.919593</td>\n",
       "      <td>0.543210</td>\n",
       "      <td>0.871287</td>\n",
       "      <td>0.669202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stacking Ensemble</td>\n",
       "      <td>0.944547</td>\n",
       "      <td>0.780822</td>\n",
       "      <td>0.564356</td>\n",
       "      <td>0.655172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.927911</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>0.638889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.942699</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.534653</td>\n",
       "      <td>0.635294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.929760</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>0.627451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.907579</td>\n",
       "      <td>0.504274</td>\n",
       "      <td>0.584158</td>\n",
       "      <td>0.541284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1-Score\n",
       "5      Voting Ensemble  0.946396   0.786667  0.584158  0.670455\n",
       "0  Logistic Regression  0.919593   0.543210  0.871287  0.669202\n",
       "6    Stacking Ensemble  0.944547   0.780822  0.564356  0.655172\n",
       "3             LightGBM  0.927911   0.600000  0.683168  0.638889\n",
       "1        Random Forest  0.942699   0.782609  0.534653  0.635294\n",
       "2              XGBoost  0.929760   0.621359  0.633663  0.627451\n",
       "4        Decision Tree  0.907579   0.504274  0.584158  0.541284"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# 1. Logistic Regression (SMOTE + Scaling)\n",
    "pipe_lr = ImbPipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "results.append(evaluate_model(\"Logistic Regression\", pipe_lr, X_test, y_test))\n",
    "\n",
    "# 2. Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "results.append(evaluate_model(\"Random Forest\", rf, X_test, y_test))\n",
    "\n",
    "# 3. XGBoost\n",
    "scale_pos = (y_train==0).sum()/(y_train==1).sum()\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=scale_pos, random_state=42)\n",
    "xgb.fit(X_train.values, y_train.values)\n",
    "results.append(evaluate_model(\"XGBoost\", xgb, X_test, y_test))\n",
    "\n",
    "# 4. LightGBM\n",
    "lgbm = LGBMClassifier(scale_pos_weight=scale_pos, random_state=42)\n",
    "lgbm.fit(X_train, y_train)\n",
    "results.append(evaluate_model(\"LightGBM\", lgbm, X_test, y_test))\n",
    "\n",
    "# 5. Decision Tree\n",
    "dt = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "results.append(evaluate_model(\"Decision Tree\", dt, X_test, y_test))\n",
    "\n",
    "# 6. Voting Ensemble\n",
    "voting = VotingClassifier(estimators=[\n",
    "    ('xgb', xgb),\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('rf', rf)\n",
    "], voting='soft')\n",
    "voting.fit(X_train, y_train)\n",
    "results.append(evaluate_model(\"Voting Ensemble\", voting, X_test, y_test))\n",
    "\n",
    "# 7. Stacking Ensemble\n",
    "stacking = StackingClassifier(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('xgb', xgb)\n",
    "], final_estimator=LogisticRegression(max_iter=1000))\n",
    "stacking.fit(X_train, y_train)\n",
    "results.append(evaluate_model(\"Stacking Ensemble\", stacking, X_test, y_test))\n",
    "\n",
    "# Summary\n",
    "pd.DataFrame(results).sort_values('F1-Score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b0d035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "Random Forest Tuned\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9698    0.9480    0.9588       981\n",
      "           1     0.5854    0.7129    0.6429       101\n",
      "\n",
      "    accuracy                         0.9261      1082\n",
      "   macro avg     0.7776    0.8304    0.8008      1082\n",
      "weighted avg     0.9339    0.9261    0.9293      1082\n",
      "\n",
      "Confusion Matrix:\n",
      " [[930  51]\n",
      " [ 29  72]]\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid,\n",
    "                       scoring='f1', cv=3, n_jobs=-1, verbose=1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "results.append(evaluate_model(\"Random Forest Tuned\", best_rf, X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a89837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "[LightGBM] [Info] Number of positive: 405, number of negative: 3923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027846 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57371\n",
      "[LightGBM] [Info] Number of data points in the train set: 4328, number of used features: 300\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.673709 -> initscore=0.725007\n",
      "[LightGBM] [Info] Start training from score 0.725007\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best params: {'class_weight': {0: 1, 1: 20}, 'learning_rate': 0.05, 'max_depth': 15, 'n_estimators': 500, 'scale_pos_weight': np.float64(19.372839506172838)}\n",
      "\n",
      "LightGBM Tuned\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9727    0.9450    0.9586       981\n",
      "           1     0.5814    0.7426    0.6522       101\n",
      "\n",
      "    accuracy                         0.9261      1082\n",
      "   macro avg     0.7771    0.8438    0.8054      1082\n",
      "weighted avg     0.9362    0.9261    0.9300      1082\n",
      "\n",
      "Confusion Matrix:\n",
      " [[927  54]\n",
      " [ 26  75]]\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [200, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'class_weight': [None, {0:1, 1:10}, {0:1, 1:20}],\n",
    "    'scale_pos_weight': [scale_pos, scale_pos*2]\n",
    "}\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "grid = GridSearchCV(lgbm, param_grid, scoring='f1', cv=3, n_jobs=-1, verbose=2)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_lgbm = grid.best_estimator_\n",
    "results.append(evaluate_model(\"LightGBM Tuned\", best_lgbm, X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f29fa153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved to: models\\models\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join('models', 'models')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save LightGBM\n",
    "joblib.dump(lgbm, os.path.join(output_dir, 'model1.pkl'))\n",
    "# Save tuned RF\n",
    "joblib.dump(best_rf, os.path.join(output_dir, 'model2.pkl'))\n",
    "\n",
    "joblib.dump(pipe_lr, os.path.join(output_dir, 'model3.pkl'))\n",
    "\n",
    "print(\"Models saved to:\", output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
